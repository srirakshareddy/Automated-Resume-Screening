{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srirakshareddy/Automated-Resume-Screening/blob/main/testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgmFs_Aej1zc",
        "outputId": "cec5018f-ad4a-48e3-f363-70560c73fa3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=1bb096fa5d4771690765e75d7341ee61ba56f1a53fff147331b1c17eec172168\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "########################### importing libraries for BERT ######################################\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data import TensorDataset\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "####################################################### Importing Libraries #######################################\n",
        "import numpy as np\n",
        "import string\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "!pip install num2words\n",
        "from num2words import num2words\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyXKgqei8rAM"
      },
      "outputs": [],
      "source": [
        "# ######################################################## data cleaning ################################################################\n",
        "\n",
        "# def data_cleaning(words): # funcion to call all other cleaning function\n",
        "#     words = convert_to_lower_case(words)\n",
        "#     words = remove_non_ascii_characters(words)\n",
        "#     # words = replace_numbers_with_words(words)\n",
        "#     words = remove_punctuation(words)\n",
        "#     words = remove_stopwords(words)\n",
        "#     # words = stem_words(words)\n",
        "#     words = lemmatize_verbs(words)\n",
        "#     words = remove_long_words(words)\n",
        "#     return words\n",
        "\n",
        "\n",
        "# def convert_to_lower_case(words): # converting string to lower case\n",
        "#     new_words = []\n",
        "#     for word in words:\n",
        "#         new_word = word.lower()   # .lower() converts string to lower case\n",
        "#         new_words.append(new_word)\n",
        "#     return new_words\n",
        "\n",
        "\n",
        "# def remove_non_ascii_characters(words): # removing non ascii characters from tokens\n",
        "#     new_words = []\n",
        "#     for word in words:\n",
        "#         string_encode = word.encode(\"ascii\", \"ignore\") #encoding string\n",
        "#         string_decode = string_encode.decode() #decoding string\n",
        "#         new_words.append(string_decode)\n",
        "#     return new_words\n",
        "\n",
        "\n",
        "# # def replace_numbers_with_words(words):\n",
        "# #     new_words = []\n",
        "# #     for word in words:\n",
        "# #         if word.isdigit():\n",
        "# #             new_word = num2words(word)\n",
        "# #             new_words.append(new_word)\n",
        "# #         else:\n",
        "# #             new_words.append(word)\n",
        "# #     return new_words\n",
        "\n",
        "\n",
        "# def remove_punctuation(words): # removing punctuations from string\n",
        "#     new_words = []\n",
        "#     for word in words:\n",
        "#         new_word = word.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "#         if new_word != '':\n",
        "#             new_words.append(new_word)\n",
        "#     return new_words\n",
        "\n",
        "\n",
        "# def remove_stopwords(words): # removing stop words\n",
        "#     new_words = []\n",
        "#     for word in words:\n",
        "#         if word not in stopwords.words('english'): # removing stop words from english language\n",
        "#             new_words.append(word)\n",
        "#     return new_words\n",
        "\n",
        "\n",
        "# # def stem_words(words):\n",
        "# #     stemmer = LancasterStemmer()\n",
        "# #     stems = []\n",
        "# #     for word in words:\n",
        "# #         stem = stemmer.stem(word)\n",
        "# #         stems.append(stem)\n",
        "# #     return stems\n",
        "\n",
        "\n",
        "# def lemmatize_verbs(words): # lemmatizing string\n",
        "#     lemmatizer = WordNetLemmatizer() # grouping together the different inflected forms of a word\n",
        "#     lemmas = []\n",
        "#     for word in words:\n",
        "#         lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "#         lemmas.append(lemma)\n",
        "#     return lemmas\n",
        "\n",
        "# def remove_long_words(words): # removing meeningless long random words \n",
        "#   new_words = []\n",
        "#   for word in words:\n",
        "#     if len(word) < 12: # words with length > 12 are removed\n",
        "#       new_words.append(word)\n",
        "#   return new_words\n",
        "\n",
        "# # def remove_non_english_words(words):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAbJ6IQB8rm1"
      },
      "outputs": [],
      "source": [
        "# ################################## creating dataframe - resume_dataset.csv ###################################\n",
        "# # df_resumes_main = pd.read_csv(\"/content/drive/MyDrive/FINAL PROJECT/CODE/Saved Models/test.csv\",index_col=\"ID\")\n",
        "# df_resumes_main = pd.read_csv(\"/content/drive/MyDrive/FINAL PROJECT/CODE/cleaned_data.csv\",index_col=\"ID\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HDgYzu-lg4eH",
        "outputId": "0ec101bd-9cdd-4e04-8c8b-202c27155794",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdN9FCaf8xmQ"
      },
      "outputs": [],
      "source": [
        "# import nltk\n",
        "# nltk.download('omw')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LDwV-128zny"
      },
      "outputs": [],
      "source": [
        "# df_resumes = df_resumes_main.Resume # extracting resumes from dataframe\n",
        "# cleaned_resumes = []\n",
        "# for resume in df_resumes:\n",
        "#     tokens = nltk.word_tokenize(resume) #tokenizing the string of resume\n",
        "#     resume = data_cleaning(tokens) #calling data cleaning function to clean the resume tokens\n",
        "#     cleaned_resumes.append(resume) # appending cleaned resume tokens into a new list\n",
        "# # print(len(cleaned_resumes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG4TAQiL83oc"
      },
      "outputs": [],
      "source": [
        "# df_resumes_main = df_resumes_main.drop('Resume', 1) # dropping the uncleaned column of resumes from dataframe\n",
        "# print(\"Dropped\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rat4_qz286hU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# flat_list = []\n",
        "\n",
        "# nltk.download('words')\n",
        "# words = set(nltk.corpus.words.words())\n",
        "\n",
        "# for resume in cleaned_resumes: # joining back tokens to make a string back\n",
        "#   flat_string = ' '.join(map(str, resume)) \n",
        "\n",
        "#   # eng_words = \" \".join(w for w in nltk.wordpunct_tokenize(flat_string) if w.lower() in words or not w.isalpha())\n",
        "\n",
        "\n",
        "#   flat_list.append(flat_string)\n",
        "\n",
        "# # final_flat_list = sorted(set(flat_list), key=lambda x:flat_list.index(x))\n",
        "\n",
        "\n",
        "# print(flat_list[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBbq9hIC880G"
      },
      "outputs": [],
      "source": [
        "# df_resumes_main['Resume'] = flat_list # adding cleaned list of resumes to the dataframe\n",
        "# print(\"Resume Updated\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUNyAEvw885Y"
      },
      "outputs": [],
      "source": [
        "# df_resumes_main.to_csv(\"cleaned_data.csv\") # saving cleaned dataframe to csv\n",
        "# print('DF Stored to CSV')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tfnsity__Qxw"
      },
      "outputs": [],
      "source": [
        "df_resumes_main = pd.read_csv(\"/content/drive/MyDrive/FINAL PROJECT/CODE/Saved Models/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEAEY4yb9A57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "c2fc7a57-ceee-49b5-f96a-1bee1fde9052"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Category                                             Resume  \\\n",
              "0        Health & Fitness  bmental healthcare nurse resume scottn68999 so...   \n",
              "1  Information Technology  b davies java programmer vancouver xc2xa0bc ja...   \n",
              "2               Designing  bbrad position graphic designer work project l...   \n",
              "3              Accountant  bdayjob ltd 120 vyse street birmingham b18 6nf...   \n",
              "4              Automobile  bxefx82xb7 aircraft satellite tv industrial ma...   \n",
              "\n",
              "   category_id  \n",
              "0            0  \n",
              "1            1  \n",
              "2            2  \n",
              "3            3  \n",
              "4            4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4f602b6a-7a1c-4762-9e85-18f2d81b9533\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Resume</th>\n",
              "      <th>category_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Health &amp; Fitness</td>\n",
              "      <td>bmental healthcare nurse resume scottn68999 so...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Information Technology</td>\n",
              "      <td>b davies java programmer vancouver xc2xa0bc ja...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Designing</td>\n",
              "      <td>bbrad position graphic designer work project l...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Accountant</td>\n",
              "      <td>bdayjob ltd 120 vyse street birmingham b18 6nf...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Automobile</td>\n",
              "      <td>bxefx82xb7 aircraft satellite tv industrial ma...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f602b6a-7a1c-4762-9e85-18f2d81b9533')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4f602b6a-7a1c-4762-9e85-18f2d81b9533 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4f602b6a-7a1c-4762-9e85-18f2d81b9533');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "col = ['Category', 'Resume'] #defining column names for dataframe\n",
        "df = df_resumes_main[col] # extracting two columns from dataframe\n",
        "df = df[pd.notnull(df['Resume'])] #Detect non-missing values\n",
        "df.columns = ['Category', 'Resume'] # column labels\n",
        "df['category_id'] = df['Category'].factorize()[0] # creating category id columns and converting category column to numbers( label encoding)\n",
        "category_id_df = df[['Category', 'category_id']].drop_duplicates().sort_values('category_id') # dropping dupicate values and sorting category_id wise\n",
        "category_to_id = dict(category_id_df.values) # converting category to id\n",
        "id_to_category = dict(category_id_df[['category_id', 'Category']].values) # id to category\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxFq4woW9Cni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3df46c8-24ec-4684-d193-9dc84de1fa3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(244, 2573)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1,2), stop_words='english') #The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings\n",
        "features = tfidf.fit_transform(df.Resume).toarray() # scalling the training data\n",
        "labels = df.category_id # generating labels\n",
        "features.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Wz5cs237oTl"
      },
      "outputs": [],
      "source": [
        "# initializing bert tokenizer \n",
        "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z5m3tIq7re1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5541213c-6449-4d36-cd99-53aae9b3ceac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# defining the model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                      num_labels=26,\n",
        "                                                      output_attentions=False,\n",
        "                                                      output_hidden_states=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B59d5VCR9XoT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "402435f6-2481-4894-a630-41566db51732"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "path = \"/content/drive/MyDrive/FINAL PROJECT/CODE/Saved Models/model20epPart2.pt\"\n",
        "model.load_state_dict(torch.load(path,map_location=torch.device('cpu')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeM5Os7H9zt9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f99e2f0b-b801-4de3-fba9-3ebe775e6489"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=26, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZriXNBLEXjO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzLSZy2Y-HSM"
      },
      "outputs": [],
      "source": [
        "# encoding the resumes using pretrained tokenizer\n",
        "test_encodings = tokenizer(df.Resume.tolist(), truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4iGeNqI_Y4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da42160-25ea-451c-ab8e-3623479395c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "test_encodings = tokenizer.batch_encode_plus(\n",
        "    df.Resume.values, \n",
        "    add_special_tokens=True, \n",
        "    return_attention_mask=True, \n",
        "    pad_to_max_length=True, \n",
        "        max_length=512,\n",
        "    return_tensors='pt'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBd7i2rf_iCs"
      },
      "outputs": [],
      "source": [
        "# getting input ids, mask, and train labels from the train_encodings\n",
        "input_ids_train = test_encodings['input_ids']\n",
        "attention_masks_train = test_encodings['attention_mask']\n",
        "labels_train = torch.tensor(np.array(labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XZXpUN9CfLio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eQbO5vJ_qU8"
      },
      "outputs": [],
      "source": [
        "# converting it to tensor dataset\n",
        "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_ids_train.size())\n",
        "print(attention_masks_train.size())\n",
        "print(labels_train.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-RHbZ3fg1IU",
        "outputId": "90a66090-97df-4861-fe9b-3235190d4eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([975, 512])\n",
            "torch.Size([975, 512])\n",
            "torch.Size([975])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EHV2J9h_uzv"
      },
      "outputs": [],
      "source": [
        "# defining the batch size and dataloader\n",
        "batch_size = 3\n",
        "dataloader_train = DataLoader(dataset_train, \n",
        "                              sampler=RandomSampler(dataset_train), \n",
        "                              batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cachTAmNAFhj"
      },
      "outputs": [],
      "source": [
        "def evaluate(dataloader_val):\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    loss_val_total = 0\n",
        "    predictions, true_vals = [], []\n",
        "    #looping for each batch in batch dataloader\n",
        "    for batch in dataloader_val:\n",
        "        device = torch.device('cpu')\n",
        "        \n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "        # specifing the input \n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels':         batch[2],\n",
        "                 }\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(**inputs)\n",
        "        # getting the loss variables     \n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        loss_val_total += loss.item()\n",
        "        # transfer the loss to cpu for calculating loss avg and return\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = inputs['labels'].cpu().numpy()\n",
        "        predictions.append(logits)\n",
        "        true_vals.append(label_ids)\n",
        "    \n",
        "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
        "    \n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    true_vals = np.concatenate(true_vals, axis=0)\n",
        "    # returning the values        \n",
        "    return loss_val_avg, predictions, true_vals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8hSr7kRAhtJ"
      },
      "outputs": [],
      "source": [
        "val_loss, predictions, true_vals = evaluate(dataloader_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8EzR55TAvcq"
      },
      "outputs": [],
      "source": [
        "# function to get f1_score\n",
        "def f1_score_func(preds, labels):\n",
        "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return f1_score(labels_flat, preds_flat, average='weighted')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IeSHwEhBBWa"
      },
      "outputs": [],
      "source": [
        "val_f1 = f1_score_func(predictions, true_vals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KPD9JEcBG1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66da02f5-7a09-40f4-ca7b-c0aad59d76c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.93025503568688\n"
          ]
        }
      ],
      "source": [
        "val_f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElORhQ6XuqBO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "175daea2-1bcc-4c2f-909b-6c16aaa28e7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([19, 19,  0,  2,  2, 20,  8, 21,  3, 12,  2,  2,  0,  0,  0, 15,  0,\n",
              "       12,  6, 23,  0, 16, 10,  0, 19, 13, 21,  8, 15,  0,  7, 20, 18,  6,\n",
              "        1, 10, 23,  9,  1, 11,  8, 19,  5, 13,  3, 23,  0,  0, 19, 20,  5,\n",
              "       24, 14,  4, 14,  8, 10,  7, 10, 24, 15,  0,  1, 24, 15,  7, 24,  5,\n",
              "       12,  6, 15,  9,  0, 17, 13,  5, 17, 20, 12, 20, 12,  3, 13,  9, 20,\n",
              "       19,  1,  0, 12, 18, 18, 15, 12,  3,  0,  0,  9, 24, 15,  7, 13,  6,\n",
              "        0,  0,  0, 13,  7, 17, 13, 23,  9,  0, 15, 13,  1,  0, 20, 13, 20,\n",
              "       13,  1,  5, 13,  6,  9, 24,  0,  1,  0,  1,  6,  8,  8,  5,  1, 19,\n",
              "       13, 24,  1, 13, 15,  0,  6,  8, 10, 21, 13,  1, 11, 15, 17, 10, 16,\n",
              "       23, 14,  2, 12, 10,  0,  1,  1, 15, 12,  4, 23, 18, 10,  6, 14,  7,\n",
              "        0, 24, 17, 15, 20,  1,  2,  7, 11, 18,  7, 12,  6, 18,  6, 13,  1,\n",
              "        6, 12,  0,  2,  0,  1,  2, 13,  1, 15,  4, 24, 13,  1, 23,  1, 15,\n",
              "       19,  3,  7, 12,  0, 10,  0, 14, 19,  0,  5, 23,  0, 22, 15,  1, 12,\n",
              "        7, 15,  8, 12,  4, 12, 22, 24, 20,  9, 20, 18,  1,  0, 10, 10,  1,\n",
              "        0, 10, 13,  9, 21, 12,  5,  0,  6, 19,  0, 12, 12, 23,  0, 19, 14,\n",
              "       12,  0,  1,  0,  7,  7,  9, 22,  9, 15, 13,  0,  9, 12,  9, 13, 15,\n",
              "        1,  8, 12,  9, 13,  6,  0, 17, 15, 13,  2,  6,  6,  0,  1, 16,  5,\n",
              "        2,  9, 12,  1, 10,  8, 15, 13,  3,  5, 21, 16, 17,  7,  1, 23,  0,\n",
              "        6, 22,  2,  3, 10, 10,  0, 20,  0,  1,  5,  0, 15, 11,  9,  0, 15,\n",
              "       13,  6, 19, 23, 17,  8, 16,  8, 15, 10, 13,  4,  4, 12, 19, 12,  8,\n",
              "       10, 13, 15,  9,  7, 17,  7, 23,  4, 12, 20,  7,  0,  5,  2, 12,  5,\n",
              "        7,  5,  9,  0, 11, 18, 13, 12,  2,  8, 20,  1, 23, 13, 23,  0,  4,\n",
              "        9,  6,  1, 10, 12,  0,  1,  6, 23, 23,  1,  1, 20, 10,  1,  7,  0,\n",
              "        2,  4,  1,  7,  1, 21,  0,  1,  0, 20, 16,  1,  8,  2, 20,  0, 21,\n",
              "        0, 17, 12, 11,  0,  1,  8, 11,  1,  0, 18, 10, 16,  6, 20, 10, 14,\n",
              "        6,  4, 18, 15,  1,  6,  0,  1,  3, 15,  1,  6,  0, 15, 19,  7, 16,\n",
              "        1,  0,  9,  0,  8,  1,  2, 23,  0,  0, 12,  1,  1,  8, 13, 23, 19,\n",
              "       13,  0, 15,  3,  1, 14,  7, 10,  5, 13, 13,  8,  5, 18,  0, 19,  0,\n",
              "        0, 13, 23,  6,  6,  8,  6, 12, 19, 18,  9, 20,  3,  8,  0,  0, 10,\n",
              "        8, 19, 24, 11,  0, 17,  8, 12, 10, 12, 13,  9, 13, 12,  8,  8,  7,\n",
              "       19, 13, 21, 10, 10,  0, 19,  1,  6,  4,  5, 12,  7, 10, 20,  1, 20,\n",
              "        6, 22, 15, 12,  0, 12,  7,  1, 20, 21,  5,  5, 15,  1,  6, 15,  8,\n",
              "       20, 14, 22,  8, 21, 15, 12, 15, 14,  8, 12, 23, 20,  8, 18, 10,  6,\n",
              "        5, 23,  8, 14,  6,  5,  7, 24,  7, 23,  7, 12,  2,  6,  7, 12, 10,\n",
              "        7,  8, 10,  9, 18,  0, 21, 20, 13,  7, 21, 19,  1,  2,  6, 18,  0,\n",
              "        1,  1, 21, 24, 19, 18, 15, 10,  0,  5,  6,  0, 15,  2, 12,  8, 10,\n",
              "        5,  7, 13, 19,  9,  6,  8, 12,  6, 13,  6,  7,  2,  5,  5, 10, 10,\n",
              "        9, 20,  8,  6, 17, 24, 23, 13, 22,  8,  7,  7,  6,  1, 12, 12,  4,\n",
              "        1,  0, 15,  6, 14,  2,  0, 12,  6,  5,  7, 15,  7, 15,  0,  1, 12,\n",
              "       16,  5, 17,  4,  8,  3, 23, 21, 12,  0, 21,  2,  2, 12, 10,  9,  7,\n",
              "       18,  0,  1, 12, 19, 10, 15, 24,  8, 23,  8, 10,  9, 23,  1,  7,  7,\n",
              "       18, 15, 18,  0,  8,  5, 12, 20, 24, 19,  8, 20,  9, 20, 15,  3,  6,\n",
              "        6, 12, 10, 10,  5, 20, 12,  8, 12,  6,  5, 23,  3, 12, 12, 15, 10,\n",
              "       10,  2,  8,  8,  0, 10, 20, 23, 19,  0, 24,  0, 19,  9,  3, 23, 12,\n",
              "       19,  1,  7,  0, 10,  2, 12, 15,  8, 10,  5,  8,  2,  1, 15, 12,  0,\n",
              "        0,  0, 10, 24,  8, 15,  7,  2, 10, 12,  4,  2, 12, 19,  9,  8, 21,\n",
              "       20, 15, 17,  1,  0, 10, 21, 12,  0, 15,  5, 15,  1, 23,  2,  0,  7,\n",
              "       12,  7, 12,  0, 24,  1,  1, 13,  8, 10, 12,  0, 23,  2, 15, 14,  2,\n",
              "        0,  1, 22,  8, 10, 12,  7,  2,  7, 10, 21,  9, 19, 20,  7, 16,  7,\n",
              "        9,  0, 12, 19,  4, 20,  6,  0, 19, 24,  1, 19,  0, 21, 10,  9, 15,\n",
              "       10,  8, 10,  7,  6, 12, 12, 12, 17, 10,  2, 23,  5,  8,  6,  7, 19,\n",
              "       17,  2,  8,  7, 20, 12, 23,  6, 19, 10, 10,  3, 11,  6,  8, 13,  1,\n",
              "        1, 18,  4, 19, 11, 12, 20, 12, 13,  4,  7,  5,  3, 23, 10, 12,  1,\n",
              "       20,  4,  9, 14, 17, 18,  1, 21, 12, 22, 19, 12,  5, 12, 17,  2,  3,\n",
              "       23, 15, 12,  9, 23, 15,  8,  8,  1,  3, 13, 23,  1,  1, 23,  8,  0,\n",
              "        0,  8,  4, 23, 15, 23, 17,  1, 10,  6,  2, 14, 12,  3,  1, 23,  0,\n",
              "        9, 12,  1,  7,  3,  6,  0, 12, 19,  8,  0, 10,  6, 13,  8,  1,  2,\n",
              "        2,  6, 12,  0,  8, 12])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "np.argmax(predictions, axis=1).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6gIhTbPup-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eeefe86-75dd-434d-952c-5287aa2258ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([19, 19,  0,  2,  2, 20,  8, 21,  3, 12,  2,  2,  0,  0, 10, 15,  0,\n",
              "       12,  6, 23,  0, 16, 10, 10, 19, 13, 21,  8, 15,  0,  7, 20, 18,  6,\n",
              "        1, 10, 23,  5,  1, 11,  8, 19,  5, 13,  3, 23,  0,  0, 19, 20,  5,\n",
              "       24,  9,  4, 14,  8, 10,  7, 10, 24, 15,  0,  1, 24, 15,  7, 24,  5,\n",
              "       12,  6, 15,  9,  0, 17, 13,  5, 17, 20, 12, 20, 12,  3, 13,  9, 20,\n",
              "       19,  0,  0, 12, 18, 18, 15, 12,  3,  0,  0,  9, 24, 15,  7, 13,  6,\n",
              "        0,  0,  0, 13,  7, 17, 13, 23,  9,  0, 15, 13,  1,  0, 20, 13,  4,\n",
              "       13,  1,  5, 13,  6,  9, 24,  0,  1,  0,  1,  6,  8,  8,  5,  2, 19,\n",
              "       13, 24,  1, 13, 15,  0,  6,  8, 10, 21, 13,  1, 11, 15, 17, 10, 16,\n",
              "       23, 14,  2, 12, 10,  0,  1,  1, 15, 12,  4, 23, 18,  2,  6, 14,  7,\n",
              "        0, 24, 17, 15, 20,  0,  2,  7, 11, 18,  7, 12,  6, 18,  6, 13,  1,\n",
              "        6, 12,  6,  2,  0,  1,  2, 13,  1, 15,  4, 24, 13,  1, 23,  1, 15,\n",
              "       19,  3,  7, 12,  0, 10,  0, 14, 19,  0,  5, 23,  6, 22, 15,  1, 12,\n",
              "        7, 15,  8,  0,  4, 12, 22, 24, 20,  9, 20, 18,  1,  0, 10, 10,  1,\n",
              "        0,  4, 13,  9, 21, 12,  5,  0,  6, 19, 12, 12, 12, 23,  0, 19, 14,\n",
              "       12,  0,  1,  0,  7,  7,  9, 22,  9, 15, 13,  0,  9, 12,  9, 13, 15,\n",
              "        1,  8, 12,  9, 13,  6,  0, 17, 15, 13,  2,  6,  6, 21,  1, 16,  5,\n",
              "        2,  9, 12,  1, 10,  8, 15, 13,  3,  5, 21, 16, 17,  7,  1, 23,  0,\n",
              "        6, 22,  2,  3, 10, 10,  1, 20,  0,  1,  5,  0, 15, 11,  9,  0, 15,\n",
              "       13,  6, 19, 23, 17,  8, 16,  8, 15, 10, 13,  4,  4, 12, 19, 12,  8,\n",
              "       10, 13, 15,  9,  7, 17,  7, 23,  4, 12, 20,  7,  8,  5,  2, 12,  5,\n",
              "        7,  5,  9,  0, 11, 18, 13, 12,  2,  8, 20,  1, 23, 13, 23,  0,  4,\n",
              "       14,  6,  1, 10, 12, 19,  1,  6, 23, 23,  1,  1, 20, 10,  1,  7,  0,\n",
              "        2,  4,  1,  7,  1, 21,  0,  1,  0, 20, 16,  1,  8,  2, 20,  0, 21,\n",
              "        0, 17, 12, 11,  0,  1,  8, 11,  1,  0, 18, 10, 16,  6, 20, 10, 14,\n",
              "        6,  4, 18, 15,  1,  6, 12,  1,  3, 15,  1,  6,  0, 15, 19,  7, 16,\n",
              "        1,  0,  9,  0,  8,  1,  2, 23,  0,  0, 12,  1,  1,  8, 13, 23, 19,\n",
              "       13,  0, 15,  3,  1, 14,  7, 10,  5, 13, 13,  8,  5, 18,  0, 19,  0,\n",
              "        0, 13, 23,  6,  6,  8,  6, 12, 19, 18,  9, 20,  3,  8,  0,  0, 10,\n",
              "        8, 19, 24, 11,  0, 17,  8, 12, 10, 12, 13,  9, 13, 12,  8,  8,  7,\n",
              "       19, 13, 21, 10, 10,  0, 19,  1,  6,  4,  5, 12,  6, 10, 20,  1, 20,\n",
              "        6, 22, 15, 12,  0, 12,  7,  1, 20, 21,  5,  5, 15,  1,  6, 15,  8,\n",
              "       20,  9, 22,  8, 21, 15, 12, 15, 14,  8, 12, 23, 20,  8, 18, 10,  6,\n",
              "        5, 23,  8,  9,  6,  5,  7, 24,  7, 23,  7, 12,  2,  6,  7, 12, 10,\n",
              "        7,  8, 10,  9, 18,  0, 21, 20, 13,  7, 21, 19,  1,  2,  6, 18,  0,\n",
              "        1,  1, 21, 24, 19, 18, 15, 10,  0,  5,  6,  0, 15,  2, 12,  8, 10,\n",
              "        5,  7, 13, 19,  9,  7,  8, 12,  6, 13,  6,  7,  2,  5,  5, 10, 10,\n",
              "        9, 20,  8,  6, 17, 24, 23, 13, 22,  8,  7,  7,  6,  1, 12, 12,  4,\n",
              "        1,  0, 15,  6, 14,  2,  0,  1,  6,  5,  7, 15,  7, 15,  0,  1, 12,\n",
              "       16,  5, 17,  4,  8,  3, 23, 21, 12, 10, 21,  2,  2, 12, 10,  9,  7,\n",
              "       18,  0,  1, 12, 19, 10, 15, 24,  8, 23,  8, 10,  9, 23,  1, 10,  7,\n",
              "       18,  4, 18,  0,  8,  5, 12, 20, 24, 19,  8, 20,  9, 20, 15,  3,  6,\n",
              "        6, 12, 10, 10,  5, 20, 12,  8, 12,  6,  5, 23,  3, 12,  0, 15, 10,\n",
              "       10,  2,  8,  8,  0, 10, 20, 23, 19,  2, 24,  0, 19,  9,  3, 23, 12,\n",
              "       19,  1,  7,  0, 10,  2, 12, 15,  8, 10,  5,  8,  2,  1, 15, 12,  0,\n",
              "        0,  0, 10, 24,  8,  2,  7,  2, 10, 12,  4,  2, 12, 19,  9,  8, 21,\n",
              "       20, 15, 17,  1,  0, 10, 21, 12,  0, 15,  5, 15,  1, 23,  2,  0,  7,\n",
              "       12,  7, 12, 13, 24,  1,  1, 13,  8, 10, 12,  0, 23,  2, 15, 14,  2,\n",
              "        0,  1, 22,  8, 10, 12,  7,  2,  7, 10, 21,  9, 19, 20,  7, 16,  7,\n",
              "        9,  0, 12, 19,  4, 20,  6,  0, 19, 24,  1, 19,  0, 21, 12,  9, 15,\n",
              "       10,  8, 10,  7,  6, 12, 12, 22, 17, 10,  2, 23,  5,  8,  6,  7, 19,\n",
              "       17,  2,  8,  7, 20, 12, 23,  6, 19, 10, 10,  3, 11,  6,  8, 13,  1,\n",
              "        1, 18,  4, 19, 11, 12, 20, 12, 13,  4,  7,  5,  3, 23, 10, 12,  1,\n",
              "       20,  4,  9, 14, 17, 18,  1, 21, 12, 22, 19, 12,  5, 12, 17,  2,  3,\n",
              "       23, 15, 18,  9, 23, 15,  8,  8,  1,  3, 13, 23,  1,  1, 23,  1,  0,\n",
              "        0,  8,  4, 23, 15, 23, 17,  1, 10,  6,  2, 14, 12,  3,  1, 23,  0,\n",
              "        9, 12,  1,  7,  3,  6,  2, 12,  4,  8, 15, 10,  6, 13,  8,  1,  2,\n",
              "        2,  6, 12,  0,  8, 12])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "true_vals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXPHoVsKup7o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}